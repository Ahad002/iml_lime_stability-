{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "LIME Stability Analysis for Text Classification\n",
        "================================================\n",
        "Team: ModelMiners (Abdul Aahad Qureshi, Khyzar Baig)\n",
        "Project: iML Winter 2025/26\n",
        "\n",
        "Research Question: How stable are LIME explanations for text classification?\n",
        "\"\"\"\n",
        "\n",
        "# Check Python version\n",
        "import sys\n",
        "print(f\"Python version: {sys.version}\")\n",
        "print(f\"Running on: Colab\")"
      ],
      "metadata": {
        "id": "kMWiGcTQ-sTO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Restart runtime after this\n",
        "!pip install transformers==4.36.0 huggingface_hub==0.20.0"
      ],
      "metadata": {
        "id": "2LMYi6bJhBnm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "# Install required packages (suppress output)\n",
        "!pip install lime==0.2.0.1\n",
        "!pip install seaborn==0.12.2\n",
        "\n",
        "# Install datasets at the desired stable version.\n",
        "# Then, upgrade huggingface_hub and fsspec to ensure compatibility with transformers\n",
        "# datasets==2.18.0 requires huggingface-hub>=0.14.0,<1.0, so an upgraded version will still be compatible.\n",
        "!pip install datasets==2.18.0\n",
        "!pip install --upgrade huggingface_hub\n",
        "!pip install --upgrade fsspec\n",
        "\n",
        "print(\" All packages installed!\")"
      ],
      "metadata": {
        "id": "hteG-FUB-tjM"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "# LIME Stability Analysis for Text Classification\n",
        "\n",
        "**Team:** ModelMiners (Abdul Aahad Qureshi, Khyzar Baig)\n",
        "**Course:** Interpretable Machine Learning (iML) Winter 2025/26\n",
        "**Supervisor:** Lukas Fehring\n",
        "\n",
        "---\n",
        "\n",
        "## 1. Introduction & Motivation\n",
        "\n",
        "LIME (Local Interpretable Model-agnostic Explanations) is one of the most widely-used methods for explaining black-box model predictions. However, it suffers from a critical flaw: **explanation instability due to random sampling**. Running LIME twice on the same input can yield different \"important features,\" undermining trust in high-stakes applications like medical diagnosis or loan decisions.\n",
        "\n",
        "**Research Question:** How stable are LIME explanations for text classification, and what factors affect this stability?\n",
        "\n",
        "---\n",
        "\n",
        "## 2. Related Work\n",
        "\n",
        "Our work builds on several key papers analyzing explainability method robustness:\n",
        "\n",
        "| Paper | Key Finding | Gap We Address |\n",
        "|-------|-------------|----------------|\n",
        "| **Ribeiro et al. (2016)** - \"Why Should I Trust You?\" | Introduced LIME | No stability analysis provided |\n",
        "| **Alvarez-Melis & Jaakkola (2018)** - \"On the Robustness of Interpretability Methods\" | First systematic stability analysis | Limited text classification focus |\n",
        "| **Slack et al. (2020)** - \"Fooling LIME and SHAP\" | Showed adversarial vulnerabilities | Focused on adversarial attacks, not hyperparameter sensitivity |\n",
        "| **Molnar et al. (2020)** - \"Interpretable Machine Learning\" | Comprehensive XAI overview | No systematic num_samples analysis |\n",
        "| **Krishna et al. (2022)** - \"The Disagreement Problem in Explainable ML\" | Showed explanation disagreement across methods | Cross-method focus, not within-method stability |\n",
        "\n",
        "**Our Contribution:** We provide the first systematic analysis of how LIME's `num_samples` parameter affects explanation stability specifically for text classification, along with analysis of sentence length and model complexity effects.\n",
        "\n",
        "---\n",
        "\n",
        "## 3. LIME Methodology for Text\n",
        "\n",
        "### 3.1 How LIME Works\n",
        "\n",
        "LIME explains predictions by fitting an interpretable surrogate model locally around a specific instance:\n",
        "\n",
        "1. **Original Prediction:** Get the black-box model's prediction for input text\n",
        "2. **Perturbation Generation:** Create neighborhood samples by randomly masking words\n",
        "3. **Surrogate Fitting:** Fit a Ridge Regression model on perturbations weighted by proximity\n",
        "4. **Explanation:** Extract feature (word) importance coefficients from surrogate\n",
        "\n",
        "### 3.2 Technical Implementation\n",
        "\n",
        "```\n",
        "Surrogate Model: Ridge Regression (L2-regularized linear model)\n",
        "Text Representation: Bag-of-Words (binary word presence)\n",
        "Perturbation Method: Random word masking (each word removed with probability p)\n",
        "Similarity Kernel: Exponential kernel based on cosine distance\n",
        "```\n",
        "\n",
        "### 3.3 Why Instability Occurs\n",
        "\n",
        "LIME's randomness comes from perturbation sampling:\n",
        "- For a sentence with n words, there are 2^n possible perturbations\n",
        "- LIME samples only `num_samples` of these (default: 5000)\n",
        "- Different random samples → Different surrogate models → Different explanations\n",
        "\n",
        "**Our Hypothesis:** Longer sentences have exponentially larger perturbation spaces (2^n), potentially causing greater instability with fixed sampling.\n",
        "\n",
        "---\n",
        "\n",
        "## 4. Experimental Setup\n",
        "\n",
        "### 4.1 Dataset\n",
        "- **SST-2** (Stanford Sentiment Treebank): Binary sentiment classification\n",
        "- ~67,000 training samples, 872 validation samples\n",
        "\n",
        "### 4.2 Models Tested\n",
        "| Model | Type | Parameters | Purpose |\n",
        "|-------|------|------------|---------|\n",
        "| Logistic Regression + TF-IDF | Simple, linear | ~10K | Baseline |\n",
        "| DistilBERT (fine-tuned) | Complex, transformer | ~66M | Real-world comparison |\n",
        "\n",
        "### 4.3 Stability Metrics\n",
        "\n",
        "| Metric | Description | Ideal Value |\n",
        "|--------|-------------|-------------|\n",
        "| **Top-K Agreement** | % overlap in top-3 important words across runs | 1.0 (perfect) |\n",
        "| **Rank Correlation** | Spearman correlation of word rankings across runs | 1.0 (perfect) |\n",
        "| **Coefficient of Variation** | Std/Mean of importance scores per word | 0.0 (no variation) |\n",
        "\n",
        "### 4.4 Reproducibility\n",
        "- **Seeds:** 5 random seeds (42, 123, 456, 789, 1000)\n",
        "- **LIME Runs:** 30 runs per sentence\n",
        "- **Results:** Reported as mean ± std across seeds\n",
        "\n",
        "---\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "QJLHqmfBbFhX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Standard libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tqdm.auto import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from scipy.stats import spearmanr\n",
        "from collections import Counter\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# ML libraries\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Explainability\n",
        "from lime.lime_text import LimeTextExplainer\n",
        "\n",
        "# Dataset\n",
        "from datasets import load_dataset\n",
        "\n",
        "# Set random seed for reproducibility\n",
        "np.random.seed(42)\n",
        "\n",
        "# Plotting style\n",
        "sns.set_style(\"whitegrid\")\n",
        "plt.rcParams['figure.figsize'] = (12, 6)\n",
        "\n",
        "print(\"All libraries imported successfully!\")"
      ],
      "metadata": {
        "id": "Zv3n3vTh-4rk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# This cell's content is now handled in hteG-FUB-tjM for better dependency management.\n",
        "# It is left empty or can be removed if desired.\n"
      ],
      "metadata": {
        "id": "i_SbBsV5_cCO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\" Downloading SST-2 dataset...\")\n",
        "\n",
        "# Load SST-2 from Hugging Face\n",
        "dataset = load_dataset(\"glue\", \"sst2\")\n",
        "\n",
        "print(f\" Dataset loaded!\")\n",
        "print(f\"   Train samples: {len(dataset['train'])}\")\n",
        "print(f\"   Test samples: {len(dataset['validation'])}\")\n",
        "\n",
        "# Convert to DataFrame\n",
        "train_df = pd.DataFrame(dataset['train'])\n",
        "test_df = pd.DataFrame(dataset['validation'])\n",
        "\n",
        "print(f\"\\n First 3 test samples:\")\n",
        "for idx, row in test_df.head(3).iterrows():\n",
        "    label = \"POSITIVE\" if row['label'] == 1 else \"NEGATIVE\"\n",
        "    print(f\"{label:10} | {row['sentence']}\")\n",
        "\n",
        "# Add sentence length\n",
        "test_df['length'] = test_df['sentence'].str.split().str.len()\n",
        "\n",
        "print(f\"\\n Sentence length statistics:\")\n",
        "print(test_df['length'].describe())"
      ],
      "metadata": {
        "id": "k2R9jzmk-8z-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SimpleLogisticModel:\n",
        "    \"\"\"Logistic Regression with TF-IDF for sentiment analysis\"\"\"\n",
        "\n",
        "    def __init__(self, seed=42):\n",
        "        self.vectorizer = TfidfVectorizer(max_features=5000, ngram_range=(1, 2))\n",
        "        self.model = LogisticRegression(random_state=seed, max_iter=1000)\n",
        "        self.is_trained = False\n",
        "\n",
        "    def fit(self, texts, labels):\n",
        "        print(\" Vectorizing text...\")\n",
        "        X = self.vectorizer.fit_transform(texts)\n",
        "        print(f\" Feature matrix: {X.shape}\")\n",
        "\n",
        "        print(\" Training model...\")\n",
        "        self.model.fit(X, labels)\n",
        "        self.is_trained = True\n",
        "\n",
        "        train_acc = accuracy_score(labels, self.model.predict(X))\n",
        "        print(f\" Training accuracy: {train_acc:.3f}\")\n",
        "\n",
        "    def predict_proba(self, texts):\n",
        "        if isinstance(texts, str):\n",
        "            texts = [texts]\n",
        "        X = self.vectorizer.transform(texts)\n",
        "        return self.model.predict_proba(X)\n",
        "\n",
        "# Train the model\n",
        "print(\" Training Sentiment Classifier\\n\")\n",
        "model = SimpleLogisticModel(seed=42)\n",
        "model.fit(train_df['sentence'].tolist(), train_df['label'].tolist())\n",
        "\n",
        "# Test accuracy\n",
        "print(\"\\n Evaluating on test set...\")\n",
        "test_preds = model.predict_proba(test_df['sentence'].tolist())\n",
        "test_acc = accuracy_score(test_df['label'].tolist(),\n",
        "                          (test_preds[:, 1] > 0.5).astype(int))\n",
        "print(f\" Test accuracy: {test_acc:.3f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cu62hpLPB24k",
        "outputId": "22d38583-4072-4e35-96cc-72e1b85a5885"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Training Sentiment Classifier\n",
            "\n",
            " Vectorizing text...\n",
            " Feature matrix: (67349, 5000)\n",
            " Training model...\n",
            " Training accuracy: 0.867\n",
            "\n",
            " Evaluating on test set...\n",
            " Test accuracy: 0.804\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class LIMEStabilityAnalyzer:\n",
        "    \"\"\"Analyze LIME explanation stability\"\"\"\n",
        "\n",
        "    def __init__(self, model):\n",
        "        self.model = model\n",
        "        self.explainer = LimeTextExplainer(class_names=['negative', 'positive'])\n",
        "\n",
        "    def explain_once(self, text, num_samples=1000):\n",
        "        \"\"\"Get single LIME explanation\"\"\"\n",
        "        exp = self.explainer.explain_instance(\n",
        "            text,\n",
        "            self.model.predict_proba,\n",
        "            num_features=10,\n",
        "            num_samples=num_samples\n",
        "        )\n",
        "        return dict(exp.as_list())\n",
        "\n",
        "    def explain_multiple(self, text, num_samples=1000, num_runs=30):\n",
        "        \"\"\"Run LIME multiple times\"\"\"\n",
        "        explanations = []\n",
        "        for _ in range(num_runs):\n",
        "            exp = self.explain_once(text, num_samples)\n",
        "            explanations.append(exp)\n",
        "        return explanations\n",
        "\n",
        "    def get_top_k(self, explanation, k=3):\n",
        "        \"\"\"Get top-k important words\"\"\"\n",
        "        sorted_words = sorted(explanation.items(),\n",
        "                             key=lambda x: abs(x[1]),\n",
        "                             reverse=True)\n",
        "        return [word for word, _ in sorted_words[:k]]\n",
        "\n",
        "    def visualize_explanation(self, explanation, title=\"LIME Explanation\"):\n",
        "        \"\"\"Plot word importances\"\"\"\n",
        "        sorted_items = sorted(explanation.items(),\n",
        "                             key=lambda x: abs(x[1]),\n",
        "                             reverse=True)[:10]\n",
        "        words, scores = zip(*sorted_items)\n",
        "\n",
        "        colors = ['red' if s < 0 else 'green' for s in scores]\n",
        "\n",
        "        plt.figure(figsize=(10, 6))\n",
        "        plt.barh(words, scores, color=colors, alpha=0.7)\n",
        "        plt.xlabel('Importance Score')\n",
        "        plt.title(title)\n",
        "        plt.axvline(x=0, color='black', linestyle='--', linewidth=0.8)\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "# Create analyzer\n",
        "analyzer = LIMEStabilityAnalyzer(model)\n",
        "print(\" LIME Analyzer ready!\")"
      ],
      "metadata": {
        "id": "RcW363AMCFaj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Test on example sentence\n",
        "test_sentence = \"This movie was absolutely terrible and boring\"\n",
        "\n",
        "print(f\" Test sentence: '{test_sentence}'\")\n",
        "print(f\"\\n Model prediction:\")\n",
        "prob = model.predict_proba([test_sentence])[0]\n",
        "pred = \"POSITIVE\" if prob[1] > 0.5 else \"NEGATIVE\"\n",
        "print(f\"   {pred} (confidence: {max(prob):.3f})\")\n",
        "\n",
        "print(f\"\\n Running LIME...\")\n",
        "explanation = analyzer.explain_once(test_sentence, num_samples=1000)\n",
        "\n",
        "# Visualize\n",
        "analyzer.visualize_explanation(explanation,\n",
        "                               title=\"LIME Explanation: Negative Review\")\n",
        "\n",
        "# Show top words\n",
        "print(f\"\\n Top 5 important words:\")\n",
        "sorted_exp = sorted(explanation.items(), key=lambda x: abs(x[1]), reverse=True)\n",
        "for word, score in sorted_exp[:5]:\n",
        "    direction = \"→ Negative\" if score < 0 else \"→ Positive\"\n",
        "    print(f\"   {word:15} {score:+.3f}  {direction}\")"
      ],
      "metadata": {
        "id": "E6steJ-QCLrg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class StabilityMetrics:\n",
        "    \"\"\"Calculate stability metrics for LIME explanations\"\"\"\n",
        "\n",
        "    @staticmethod\n",
        "    def top_k_agreement(explanations, k=3):\n",
        "        \"\"\"What % of top-k words overlap across runs?\"\"\"\n",
        "        top_k_lists = []\n",
        "        for exp in explanations:\n",
        "            sorted_words = sorted(exp.items(),\n",
        "                                 key=lambda x: abs(x[1]),\n",
        "                                 reverse=True)\n",
        "            top_k_lists.append(set([w for w, _ in sorted_words[:k]]))\n",
        "\n",
        "        # Pairwise overlap\n",
        "        agreements = []\n",
        "        n = len(top_k_lists)\n",
        "        for i in range(n):\n",
        "            for j in range(i+1, n):\n",
        "                overlap = len(top_k_lists[i] & top_k_lists[j]) / k\n",
        "                agreements.append(overlap)\n",
        "\n",
        "        return np.mean(agreements) if agreements else 0.0\n",
        "\n",
        "    @staticmethod\n",
        "    def rank_correlation(explanations):\n",
        "        \"\"\"Average Spearman correlation of word rankings\"\"\"\n",
        "        # Get all unique words\n",
        "        all_words = set()\n",
        "        for exp in explanations:\n",
        "            all_words.update(exp.keys())\n",
        "        all_words = sorted(list(all_words))\n",
        "\n",
        "        # Create rank vectors\n",
        "        rank_vectors = []\n",
        "        for exp in explanations:\n",
        "            ranks = []\n",
        "            sorted_words = sorted(exp.items(),\n",
        "                                 key=lambda x: abs(x[1]),\n",
        "                                 reverse=True)\n",
        "            word_to_rank = {w: i for i, (w, _) in enumerate(sorted_words)}\n",
        "\n",
        "            for word in all_words:\n",
        "                ranks.append(word_to_rank.get(word, len(sorted_words)))\n",
        "            rank_vectors.append(ranks)\n",
        "\n",
        "        # Pairwise correlations\n",
        "        correlations = []\n",
        "        n = len(rank_vectors)\n",
        "        for i in range(n):\n",
        "            for j in range(i+1, n):\n",
        "                corr, _ = spearmanr(rank_vectors[i], rank_vectors[j])\n",
        "                if not np.isnan(corr):\n",
        "                    correlations.append(corr)\n",
        "\n",
        "        return np.mean(correlations) if correlations else 0.0\n",
        "\n",
        "    @staticmethod\n",
        "    def coefficient_of_variation(explanations):\n",
        "        \"\"\"How much do importance scores vary?\"\"\"\n",
        "        all_words = set()\n",
        "        for exp in explanations:\n",
        "            all_words.update(exp.keys())\n",
        "\n",
        "        cvs = []\n",
        "        for word in all_words:\n",
        "            scores = [exp.get(word, 0) for exp in explanations]\n",
        "            mean_abs = np.mean(np.abs(scores))\n",
        "            std = np.std(scores)\n",
        "\n",
        "            if mean_abs > 0:\n",
        "                cvs.append(std / mean_abs)\n",
        "\n",
        "        return np.mean(cvs) if cvs else 0.0\n",
        "\n",
        "metrics_calc = StabilityMetrics()\n",
        "print(\" Stability metrics ready!\")"
      ],
      "metadata": {
        "id": "D3b3-BgmCXgJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "## 5. Experiment 1: Effect of num_samples on Stability\n",
        "\n",
        "**Research Question:** How many perturbation samples does LIME need for stable explanations?\n",
        "\n",
        "**Hypothesis:** More samples → More stable explanations (but diminishing returns)\n",
        "\n",
        "**Setup:**\n",
        "- 50 sentences from SST-2 test set\n",
        "- num_samples ∈ {100, 250, 500, 1000, 2000}\n",
        "- 30 LIME runs per sentence × 5 seeds = 150 total runs per sentence\n",
        "\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "CGDGi6a9bNYi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "SEEDS = [42, 123, 456, 789, 1000]  # 5 seeds as proposed\n",
        "\n",
        "# =============================================================================\n",
        "# EXPERIMENT 1: How does num_samples affect stability?\n",
        "# Proposal: 50 sentences, num_samples ∈ {100, 250, 500, 1000, 2000}, 30 runs\n",
        "# =============================================================================\n",
        "\n",
        "print(\"=\"*70)\n",
        "print(\" EXPERIMENT 1: How does num_samples affect stability?\")\n",
        "print(\"=\"*70)\n",
        "print(f\" Settings: 50 sentences | 5 seeds | 30 LIME runs per sentence\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# Settings (matching proposal)\n",
        "num_samples_list = [100, 250, 500, 1000, 2000]\n",
        "n_sentences = 50      # Proposal: 50 sentences\n",
        "n_runs = 30           # Proposal: 30 runs\n",
        "\n",
        "exp1_all_seeds = []\n",
        "\n",
        "for seed in SEEDS:\n",
        "    print(f\"\\n{'='*70}\")\n",
        "    print(f\" Running with SEED = {seed}\")\n",
        "    print(f\"{'='*70}\")\n",
        "\n",
        "    np.random.seed(seed)\n",
        "\n",
        "    # Get sample sentences for this seed\n",
        "    sample_sentences = test_df.sample(n=n_sentences, random_state=seed)\n",
        "\n",
        "    seed_results = []\n",
        "\n",
        "    for num_samples in tqdm(num_samples_list, desc=f\"num_samples (seed={seed})\"):\n",
        "        print(f\"\\n  Testing num_samples = {num_samples}\")\n",
        "\n",
        "        top3_list = []\n",
        "        corr_list = []\n",
        "        cv_list = []\n",
        "\n",
        "        for idx, row in tqdm(sample_sentences.iterrows(),\n",
        "                             total=len(sample_sentences),\n",
        "                             desc=f\"Sentences\", leave=False):\n",
        "            text = row['sentence']\n",
        "\n",
        "            # Run LIME multiple times\n",
        "            explanations = analyzer.explain_multiple(text,\n",
        "                                                     num_samples=num_samples,\n",
        "                                                     num_runs=n_runs)\n",
        "\n",
        "            # Calculate metrics\n",
        "            top3 = metrics_calc.top_k_agreement(explanations, k=3)\n",
        "            corr = metrics_calc.rank_correlation(explanations)\n",
        "            cv = metrics_calc.coefficient_of_variation(explanations)\n",
        "\n",
        "            top3_list.append(top3)\n",
        "            corr_list.append(corr)\n",
        "            cv_list.append(cv)\n",
        "\n",
        "        seed_results.append({\n",
        "            'seed': seed,\n",
        "            'num_samples': num_samples,\n",
        "            'top3_agreement': np.mean(top3_list),\n",
        "            'rank_correlation': np.mean(corr_list),\n",
        "            'coeff_variation': np.mean(cv_list)\n",
        "        })\n",
        "\n",
        "        print(f\"    Top-3: {np.mean(top3_list):.3f} | Corr: {np.mean(corr_list):.3f} | CV: {np.mean(cv_list):.3f}\")\n",
        "\n",
        "    exp1_all_seeds.extend(seed_results)\n",
        "\n",
        "# Convert to DataFrame and aggregate across seeds\n",
        "exp1_raw_df = pd.DataFrame(exp1_all_seeds)\n",
        "\n",
        "# Average across seeds\n",
        "exp1_results = exp1_raw_df.groupby('num_samples').agg({\n",
        "    'top3_agreement': ['mean', 'std'],\n",
        "    'rank_correlation': ['mean', 'std'],\n",
        "    'coeff_variation': ['mean', 'std']\n",
        "}).reset_index()\n",
        "\n",
        "# Flatten column names\n",
        "exp1_results.columns = ['num_samples',\n",
        "                        'top3_mean', 'top3_std',\n",
        "                        'corr_mean', 'corr_std',\n",
        "                        'cv_mean', 'cv_std']\n",
        "\n",
        "print(f\"\\n{'='*70}\")\n",
        "print(\" EXPERIMENT 1 RESULTS (Averaged across 5 seeds):\")\n",
        "print(f\"{'='*70}\")\n",
        "print(exp1_results.to_string(index=False))\n",
        "\n",
        "# Save results\n",
        "exp1_raw_df.to_csv('exp1_raw_results.csv', index=False)\n",
        "exp1_results.to_csv('exp1_aggregated_results.csv', index=False)\n",
        "print(\"\\n Results saved!\")"
      ],
      "metadata": {
        "id": "EDV7j__CCa_R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
        "\n",
        "# Plot 1: Top-3 Agreement\n",
        "axes[0].errorbar(exp1_results['num_samples'], exp1_results['top3_mean'],\n",
        "                 yerr=exp1_results['top3_std'], marker='o', linewidth=2,\n",
        "                 markersize=8, color='#2E86AB', capsize=5, capthick=2)\n",
        "axes[0].set_xlabel('Number of Samples', fontsize=12)\n",
        "axes[0].set_ylabel('Top-3 Agreement', fontsize=12)\n",
        "axes[0].set_title('Top-3 Feature Agreement vs num_samples', fontsize=14, fontweight='bold')\n",
        "axes[0].set_ylim(0, 1.0)\n",
        "axes[0].axhline(y=0.9, color='green', linestyle='--', alpha=0.5, label='High Stability (0.9)')\n",
        "axes[0].legend()\n",
        "axes[0].grid(True, alpha=0.3)\n",
        "\n",
        "# Plot 2: Rank Correlation\n",
        "axes[1].errorbar(exp1_results['num_samples'], exp1_results['corr_mean'],\n",
        "                 yerr=exp1_results['corr_std'], marker='s', linewidth=2,\n",
        "                 markersize=8, color='#A23B72', capsize=5, capthick=2)\n",
        "axes[1].set_xlabel('Number of Samples', fontsize=12)\n",
        "axes[1].set_ylabel('Spearman Rank Correlation', fontsize=12)\n",
        "axes[1].set_title('Rank Correlation vs num_samples', fontsize=14, fontweight='bold')\n",
        "axes[1].set_ylim(0, 1.0)\n",
        "axes[1].axhline(y=0.9, color='green', linestyle='--', alpha=0.5, label='High Stability (0.9)')\n",
        "axes[1].legend()\n",
        "axes[1].grid(True, alpha=0.3)\n",
        "\n",
        "# Plot 3: Coefficient of Variation\n",
        "axes[2].errorbar(exp1_results['num_samples'], exp1_results['cv_mean'],\n",
        "                 yerr=exp1_results['cv_std'], marker='^', linewidth=2,\n",
        "                 markersize=8, color='#F18F01', capsize=5, capthick=2)\n",
        "axes[2].set_xlabel('Number of Samples', fontsize=12)\n",
        "axes[2].set_ylabel('Coefficient of Variation', fontsize=12)\n",
        "axes[2].set_title('Score Variation vs num_samples', fontsize=14, fontweight='bold')\n",
        "axes[2].axhline(y=0.5, color='green', linestyle='--', alpha=0.5, label='Low Variation (0.5)')\n",
        "axes[2].legend()\n",
        "axes[2].grid(True, alpha=0.3)\n",
        "\n",
        "plt.suptitle('EXPERIMENT 1: Effect of num_samples on LIME Stability (5 seeds, 50 sentences)',\n",
        "             fontsize=16, fontweight='bold', y=1.02)\n",
        "plt.tight_layout()\n",
        "plt.savefig('exp1_samples_vs_stability.png', dpi=300, bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "print(\" Visualization saved to 'exp1_samples_vs_stability.png'\")"
      ],
      "metadata": {
        "id": "hifMGRcGEamZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(1)  # Your exp1 figure\n",
        "plt.savefig('results/exp1_samples.png', dpi=300, bbox_inches='tight')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "edLtkXKQbP0m",
        "outputId": "c7485ef3-74ed-4cd6-a44b-d0e5b63fdbf8"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1200x600 with 0 Axes>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.patches as mpatches\n",
        "from matplotlib.patches import FancyBboxPatch, Wedge, Circle, Rectangle\n",
        "import numpy as np\n",
        "\n",
        "# Set style\n",
        "plt.rcParams['font.family'] = 'sans-serif'\n",
        "plt.rcParams['font.size'] = 12\n",
        "plt.rcParams['axes.spines.top'] = False\n",
        "plt.rcParams['axes.spines.right'] = False\n",
        "plt.rcParams['axes.linewidth'] = 2\n",
        "\n",
        "# Color palette\n",
        "COLORS = {\n",
        "    'red': '#E74C3C',\n",
        "    'dark_red': '#C0392B',\n",
        "    'orange': '#F39C12',\n",
        "    'yellow': '#F1C40F',\n",
        "    'green': '#27AE60',\n",
        "    'dark_green': '#1E8449',\n",
        "    'blue': '#3498DB',\n",
        "    'dark_blue': '#1A5276',\n",
        "    'purple': '#9B59B6',\n",
        "    'teal': '#17A589',\n",
        "    'gray': '#7F8C8D',\n",
        "    'light_gray': '#ECF0F1',\n",
        "    'dark': '#2C3E50',\n",
        "    'white': '#FFFFFF'\n",
        "}\n",
        "fig1, ax1 = plt.subplots(figsize=(10, 6))\n",
        "\n",
        "num_samples = [100, 250, 500, 1000, 2000]\n",
        "agreement = [0.856, 0.910, 0.933, 0.949, 0.962]\n",
        "std_dev = [0.020, 0.011, 0.015, 0.012, 0.011]\n",
        "\n",
        "# Create area fill with gradient effect\n",
        "ax1.fill_between(range(len(num_samples)), agreement, 0.8, alpha=0.3, color=COLORS['blue'])\n",
        "ax1.fill_between(range(len(num_samples)), agreement, 1.0, alpha=0.2, color=COLORS['red'])\n",
        "\n",
        "# Plot line with markers\n",
        "ax1.plot(range(len(num_samples)), agreement, 'o-', color=COLORS['dark_blue'],\n",
        "         linewidth=3, markersize=15, markerfacecolor=COLORS['blue'],\n",
        "         markeredgecolor=COLORS['dark_blue'], markeredgewidth=2)\n",
        "\n",
        "# Error bars\n",
        "ax1.errorbar(range(len(num_samples)), agreement, yerr=std_dev, fmt='none',\n",
        "             color=COLORS['dark_blue'], capsize=8, capthick=2, linewidth=2)\n",
        "\n",
        "# Add value labels above each point\n",
        "for i, (val, err) in enumerate(zip(agreement, std_dev)):\n",
        "    ax1.annotate(f'{val:.2f}', (i, val + err + 0.015), ha='center', va='bottom',\n",
        "                fontsize=14, fontweight='bold', color=COLORS['dark_blue'])\n",
        "\n",
        "# Reference lines\n",
        "ax1.axhline(y=1.0, color=COLORS['green'], linestyle='--', linewidth=2, label='Perfect (1.0)')\n",
        "ax1.axhline(y=0.95, color=COLORS['orange'], linestyle=':', linewidth=2, label='Acceptable (0.95)')\n",
        "\n",
        "# Highlight the gap at the end\n",
        "ax1.annotate('', xy=(4, 1.0), xytext=(4, 0.962),\n",
        "            arrowprops=dict(arrowstyle='<->', color=COLORS['red'], lw=2))\n",
        "ax1.text(4.3, 0.98, 'Still 4%\\nUnreliable!', fontsize=11, fontweight='bold',\n",
        "         color=COLORS['red'], va='center')\n",
        "\n",
        "# Styling\n",
        "ax1.set_xlim(-0.5, 4.5)\n",
        "ax1.set_ylim(0.82, 1.05)\n",
        "ax1.set_xticks(range(len(num_samples)))\n",
        "ax1.set_xticklabels([str(n) for n in num_samples], fontsize=12, fontweight='bold')\n",
        "ax1.set_xlabel('Number of Perturbation Samples', fontsize=14, fontweight='bold')\n",
        "ax1.set_ylabel('Top-3 Agreement', fontsize=14, fontweight='bold')\n",
        "ax1.set_title('Exp 1: More Samples Improve Stability, But Never Reach 100%\\n(p < 0.001, ANOVA)',\n",
        "              fontsize=16, fontweight='bold', color=COLORS['dark_blue'], pad=15)\n",
        "ax1.legend(loc='lower right', fontsize=11)\n",
        "\n",
        "# Add colored region labels\n",
        "ax1.text(2, 0.85, 'RELIABLE ZONE', fontsize=10, ha='center', color=COLORS['blue'],\n",
        "         fontweight='bold', alpha=0.7)\n",
        "ax1.text(2, 0.99, 'UNRELIABLE ZONE', fontsize=10, ha='center', color=COLORS['red'],\n",
        "         fontweight='bold', alpha=0.7)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('exp1_num_samples.png', dpi=300, bbox_inches='tight', facecolor='white')\n",
        "plt.close()\n",
        "print(\"✓ Saved: exp1_num_samples.png\")"
      ],
      "metadata": {
        "id": "995TbKsTakFZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "## 6. Experiment 2: Effect of Sentence Length on Stability\n",
        "\n",
        "**Research Question:** Does input complexity (sentence length) affect LIME stability?\n",
        "\n",
        "**Hypothesis:** Longer sentences create larger perturbation spaces (2^n combinations). With fixed `num_samples`, this may cause instability as we sample a smaller fraction of the space.\n",
        "\n",
        "| Sentence Length | Words (n) | Perturbation Space (2^n) | Coverage at num_samples=1000 |\n",
        "|-----------------|-----------|--------------------------|------------------------------|\n",
        "| Short (≤7) | 5 | 32 | 100% (oversampled) |\n",
        "| Medium (8-15) | 10 | 1,024 | ~98% |\n",
        "| Long (>15) | 20 | 1,048,576 | ~0.1% |\n",
        "\n",
        "**Setup:**\n",
        "- 15 sentences per length group × 5 seeds\n",
        "- num_samples = 1000 (fixed)\n",
        "- 30 LIME runs per sentence\n",
        "\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "cSXkXP4pbUYU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\" EXPERIMENT 2: Does sentence length affect stability?\")\n",
        "print(\"=\"*70)\n",
        "print(f\" Settings: 15 sentences per group | 5 seeds | 30 LIME runs | 1000 samples\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "n_per_group = 15\n",
        "num_samples = 1000\n",
        "n_runs = 30\n",
        "\n",
        "exp2_all_seeds = []\n",
        "\n",
        "for seed in SEEDS:\n",
        "    print(f\"\\n{'='*70}\")\n",
        "    print(f\" Running with SEED = {seed}\")\n",
        "    print(f\"{'='*70}\")\n",
        "\n",
        "    np.random.seed(seed)\n",
        "\n",
        "    # Group sentences by length\n",
        "    short = test_df[test_df['length'] <= 7].sample(n=n_per_group, random_state=seed)\n",
        "    medium = test_df[(test_df['length'] > 7) & (test_df['length'] <= 15)].sample(n=n_per_group, random_state=seed)\n",
        "    long = test_df[test_df['length'] > 15].sample(n=n_per_group, random_state=seed)\n",
        "\n",
        "    groups = {\n",
        "        'Short (≤7 words)': short,\n",
        "        'Medium (8-15 words)': medium,\n",
        "        'Long (>15 words)': long\n",
        "    }\n",
        "\n",
        "    for group_name, group_df in groups.items():\n",
        "        print(f\"\\n   {group_name}\")\n",
        "\n",
        "        top3_list = []\n",
        "        corr_list = []\n",
        "        cv_list = []\n",
        "\n",
        "        for idx, row in tqdm(group_df.iterrows(), total=len(group_df),\n",
        "                             desc=group_name, leave=False):\n",
        "            explanations = analyzer.explain_multiple(row['sentence'],\n",
        "                                                     num_samples=num_samples,\n",
        "                                                     num_runs=n_runs)\n",
        "\n",
        "            top3 = metrics_calc.top_k_agreement(explanations, k=3)\n",
        "            corr = metrics_calc.rank_correlation(explanations)\n",
        "            cv = metrics_calc.coefficient_of_variation(explanations)\n",
        "\n",
        "            top3_list.append(top3)\n",
        "            corr_list.append(corr)\n",
        "            cv_list.append(cv)\n",
        "\n",
        "        exp2_all_seeds.append({\n",
        "            'seed': seed,\n",
        "            'group': group_name,\n",
        "            'top3_agreement': np.mean(top3_list),\n",
        "            'rank_correlation': np.mean(corr_list),\n",
        "            'coeff_variation': np.mean(cv_list),\n",
        "            'avg_length': group_df['length'].mean()\n",
        "        })\n",
        "\n",
        "        print(f\"    Top-3: {np.mean(top3_list):.3f} | Corr: {np.mean(corr_list):.3f} | CV: {np.mean(cv_list):.3f}\")\n",
        "\n",
        "# Aggregate across seeds\n",
        "exp2_raw_df = pd.DataFrame(exp2_all_seeds)\n",
        "\n",
        "exp2_results = exp2_raw_df.groupby('group').agg({\n",
        "    'top3_agreement': ['mean', 'std'],\n",
        "    'rank_correlation': ['mean', 'std'],\n",
        "    'coeff_variation': ['mean', 'std'],\n",
        "    'avg_length': 'mean'\n",
        "}).reset_index()\n",
        "\n",
        "exp2_results.columns = ['group', 'top3_mean', 'top3_std', 'corr_mean', 'corr_std',\n",
        "                        'cv_mean', 'cv_std', 'avg_length']\n",
        "\n",
        "# Reorder groups\n",
        "group_order = ['Short (≤7 words)', 'Medium (8-15 words)', 'Long (>15 words)']\n",
        "exp2_results['group'] = pd.Categorical(exp2_results['group'], categories=group_order, ordered=True)\n",
        "exp2_results = exp2_results.sort_values('group')\n",
        "\n",
        "print(f\"\\n{'='*70}\")\n",
        "print(\" EXPERIMENT 2 RESULTS (Averaged across 5 seeds):\")\n",
        "print(f\"{'='*70}\")\n",
        "print(exp2_results.to_string(index=False))\n",
        "\n",
        "# Save\n",
        "exp2_raw_df.to_csv('exp2_raw_results.csv', index=False)\n",
        "exp2_results.to_csv('exp2_aggregated_results.csv', index=False)"
      ],
      "metadata": {
        "id": "-mI1v-zFNAZw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
        "\n",
        "colors = ['#2E86AB', '#A23B72', '#F18F01']\n",
        "group_names = exp2_results['group'].tolist()\n",
        "x_pos = np.arange(len(group_names))\n",
        "\n",
        "# Plot 1: Top-3 Agreement\n",
        "bars1 = axes[0].bar(x_pos, exp2_results['top3_mean'], yerr=exp2_results['top3_std'],\n",
        "                    color=colors, edgecolor='black', linewidth=1.2, capsize=5)\n",
        "axes[0].set_xticks(x_pos)\n",
        "axes[0].set_xticklabels(group_names, rotation=15, ha='right')\n",
        "axes[0].set_ylabel('Top-3 Agreement', fontsize=12)\n",
        "axes[0].set_title('Top-3 Agreement vs Sentence Length', fontsize=14, fontweight='bold')\n",
        "axes[0].set_ylim(0, 1.1)\n",
        "axes[0].axhline(y=0.9, color='green', linestyle='--', alpha=0.5)\n",
        "\n",
        "for i, (bar, val) in enumerate(zip(bars1, exp2_results['top3_mean'])):\n",
        "    axes[0].text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.05,\n",
        "                 f'{val:.3f}', ha='center', va='bottom', fontsize=10, fontweight='bold')\n",
        "\n",
        "# Plot 2: Rank Correlation\n",
        "bars2 = axes[1].bar(x_pos, exp2_results['corr_mean'], yerr=exp2_results['corr_std'],\n",
        "                    color=colors, edgecolor='black', linewidth=1.2, capsize=5)\n",
        "axes[1].set_xticks(x_pos)\n",
        "axes[1].set_xticklabels(group_names, rotation=15, ha='right')\n",
        "axes[1].set_ylabel('Rank Correlation', fontsize=12)\n",
        "axes[1].set_title('Rank Correlation vs Sentence Length', fontsize=14, fontweight='bold')\n",
        "axes[1].set_ylim(0, 1.1)\n",
        "axes[1].axhline(y=0.9, color='green', linestyle='--', alpha=0.5)\n",
        "\n",
        "for i, (bar, val) in enumerate(zip(bars2, exp2_results['corr_mean'])):\n",
        "    axes[1].text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.05,\n",
        "                 f'{val:.3f}', ha='center', va='bottom', fontsize=10, fontweight='bold')\n",
        "\n",
        "# Plot 3: Coefficient of Variation\n",
        "bars3 = axes[2].bar(x_pos, exp2_results['cv_mean'], yerr=exp2_results['cv_std'],\n",
        "                    color=colors, edgecolor='black', linewidth=1.2, capsize=5)\n",
        "axes[2].set_xticks(x_pos)\n",
        "axes[2].set_xticklabels(group_names, rotation=15, ha='right')\n",
        "axes[2].set_ylabel('Coefficient of Variation', fontsize=12)\n",
        "axes[2].set_title('Score Variation vs Sentence Length', fontsize=14, fontweight='bold')\n",
        "axes[2].axhline(y=0.5, color='green', linestyle='--', alpha=0.5)\n",
        "\n",
        "for i, (bar, val) in enumerate(zip(bars3, exp2_results['cv_mean'])):\n",
        "    axes[2].text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.05,\n",
        "                 f'{val:.3f}', ha='center', va='bottom', fontsize=10, fontweight='bold')\n",
        "\n",
        "plt.suptitle('EXPERIMENT 2: Effect of Sentence Length on LIME Stability (5 seeds)',\n",
        "             fontsize=16, fontweight='bold', y=1.02)\n",
        "plt.tight_layout()\n",
        "plt.savefig('exp2_length_vs_stability.png', dpi=300, bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "print(\" Visualization saved to 'exp2_length_vs_stability.png'\")\n"
      ],
      "metadata": {
        "id": "LsfHa7lpO93a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fig2, ax2 = plt.subplots(figsize=(10, 6))\n",
        "\n",
        "lengths = ['Short (≤7 words)', 'Medium (8-15 words)', 'Long (>15 words)']\n",
        "agreement_len = [0.932, 0.973, 0.956]\n",
        "cv_len = [0.229, 0.417, 0.744]\n",
        "std_len = [0.022, 0.015, 0.020]\n",
        "\n",
        "x = np.arange(len(lengths))\n",
        "width = 0.35\n",
        "\n",
        "# Create bars with different styles\n",
        "bars1 = ax2.bar(x - width/2, agreement_len, width, yerr=std_len,\n",
        "                color=[COLORS['orange'], COLORS['green'], COLORS['orange']],\n",
        "                edgecolor=[COLORS['dark_red'], COLORS['dark_green'], COLORS['dark_red']],\n",
        "                linewidth=3, capsize=6, label='Top-3 Agreement', hatch=['', '', ''])\n",
        "\n",
        "bars2 = ax2.bar(x + width/2, cv_len, width,\n",
        "                color=[COLORS['teal'], COLORS['teal'], COLORS['red']],\n",
        "                edgecolor=COLORS['dark'], linewidth=2, label='Score Variation (CV)', alpha=0.8)\n",
        "\n",
        "# Add value labels\n",
        "for bar, val in zip(bars1, agreement_len):\n",
        "    ax2.text(bar.get_x() + bar.get_width()/2, val + 0.05, f'{val:.2f}',\n",
        "             ha='center', va='bottom', fontsize=13, fontweight='bold', color=COLORS['dark'])\n",
        "\n",
        "for bar, val in zip(bars2, cv_len):\n",
        "    color = COLORS['red'] if val > 0.5 else COLORS['dark']\n",
        "    ax2.text(bar.get_x() + bar.get_width()/2, val + 0.02, f'{val:.2f}',\n",
        "             ha='center', va='bottom', fontsize=13, fontweight='bold', color=color)\n",
        "\n",
        "# Highlight best (Medium)\n",
        "ax2.annotate('✓ BEST', xy=(1 - width/2, 0.973), xytext=(0.3, 1.05),\n",
        "            fontsize=14, fontweight='bold', color=COLORS['green'],\n",
        "            arrowprops=dict(arrowstyle='->', color=COLORS['green'], lw=2))\n",
        "\n",
        "# Highlight worst CV (Long)\n",
        "ax2.annotate('3x MORE\\nVARIATION!', xy=(2 + width/2, 0.744), xytext=(2.6, 0.85),\n",
        "            fontsize=11, fontweight='bold', color=COLORS['red'],\n",
        "            arrowprops=dict(arrowstyle='->', color=COLORS['red'], lw=2),\n",
        "            bbox=dict(boxstyle='round,pad=0.3', facecolor='#FADBD8', edgecolor=COLORS['red']))\n",
        "\n",
        "# Reference line\n",
        "ax2.axhline(y=1.0, color=COLORS['gray'], linestyle='--', linewidth=1.5, alpha=0.5)\n",
        "\n",
        "# Styling\n",
        "ax2.set_ylim(0, 1.15)\n",
        "ax2.set_xticks(x)\n",
        "ax2.set_xticklabels(lengths, fontsize=12, fontweight='bold')\n",
        "ax2.set_ylabel('Score', fontsize=14, fontweight='bold')\n",
        "ax2.set_title('Exp 2: Medium-Length Sentences Are Most Stable\\n(p = 0.018, ANOVA)',\n",
        "              fontsize=16, fontweight='bold', color=COLORS['dark_blue'], pad=15)\n",
        "ax2.legend(loc='upper left', fontsize=11, framealpha=0.9)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('exp2_sentence_length.png', dpi=300, bbox_inches='tight', facecolor='white')\n",
        "plt.close()\n",
        "print(\"✓ Saved: exp2_sentence_length.png\")"
      ],
      "metadata": {
        "id": "dgZ_FtXdfah8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "## 7. Experiment 3: Effect of Model Complexity on Stability\n",
        "\n",
        "**Research Question:** Do complex models (DistilBERT) produce less stable LIME explanations than simple models (Logistic Regression)?\n",
        "\n",
        "**Hypothesis:** Complex models have more non-linear decision boundaries, making local linear approximations (LIME's surrogate) less accurate and potentially less stable.\n",
        "\n",
        "| Model | Decision Boundary | Expected Stability |\n",
        "|-------|-------------------|-------------------|\n",
        "| Logistic Regression | Linear | High (easy to approximate) |\n",
        "| DistilBERT | Highly non-linear | Lower (harder to approximate) |\n",
        "\n",
        "**Setup:**\n",
        "- Same 15 sentences for both models × 5 seeds\n",
        "- num_samples = 1000 (fixed)\n",
        "- 30 LIME runs per sentence\n",
        "\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "SQWavSsSbar4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Install transformer library for DistilBERT\n",
        "print(\" Installing transformers library...\")\n",
        "!pip install transformers torch -q\n",
        "\n",
        "print(\"Transformers installed!\")"
      ],
      "metadata": {
        "id": "sFlgkG5YivtQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "import torch\n",
        "\n",
        "class DistilBERTModel:\n",
        "    \"\"\"DistilBERT model for sentiment classification\"\"\"\n",
        "\n",
        "    def __init__(self, model_name=\"distilbert/distilbert-base-uncased-finetuned-sst-2-english\"):\n",
        "        \"\"\"\n",
        "        Initialize pre-trained DistilBERT\n",
        "        \"\"\"\n",
        "        print(f\" Loading {model_name}...\")\n",
        "\n",
        "        try:\n",
        "            # Load tokenizer - NO trust_remote_code needed!\n",
        "            self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "            # Load pre-trained model\n",
        "            self.model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\" Error loading {model_name}: {e}\")\n",
        "            print(\" Trying alternative model...\")\n",
        "\n",
        "            # Alternative: use a different SST-2 model\n",
        "            alternative_name = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
        "            self.tokenizer = AutoTokenizer.from_pretrained(alternative_name)\n",
        "            self.model = AutoModelForSequenceClassification.from_pretrained(alternative_name)\n",
        "\n",
        "        self.model.eval()\n",
        "        self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "        self.model.to(self.device)\n",
        "\n",
        "        print(f\" Model loaded on {self.device}\")\n",
        "        self.is_trained = True\n",
        "\n",
        "    def predict_proba(self, texts):\n",
        "        \"\"\"Predict probabilities for text(s)\"\"\"\n",
        "        if isinstance(texts, str):\n",
        "            texts = [texts]\n",
        "\n",
        "        inputs = self.tokenizer(\n",
        "            texts,\n",
        "            return_tensors=\"pt\",\n",
        "            padding=True,\n",
        "            truncation=True,\n",
        "            max_length=512\n",
        "        )\n",
        "\n",
        "        inputs = {k: v.to(self.device) for k, v in inputs.items()}\n",
        "\n",
        "        with torch.no_grad():\n",
        "            outputs = self.model(**inputs)\n",
        "            probs = torch.nn.functional.softmax(outputs.logits, dim=-1)\n",
        "\n",
        "        return probs.cpu().numpy()\n",
        "\n",
        "# Create and test\n",
        "print(\" Creating DistilBERT Model\\n\")\n",
        "bert_model = DistilBERTModel()\n",
        "\n",
        "test_text = \"This movie was absolutely terrible and boring\"\n",
        "bert_prob = bert_model.predict_proba([test_text])[0]\n",
        "bert_pred = \"POSITIVE\" if bert_prob[1] > 0.5 else \"NEGATIVE\"\n",
        "print(f\"\\n Test: '{test_text}'\")\n",
        "print(f\" Prediction: {bert_pred} (confidence: {max(bert_prob):.3f})\")"
      ],
      "metadata": {
        "id": "THT4Y3XNi2kp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\" EXPERIMENT 3: Does model complexity affect stability?\")\n",
        "print(\"=\"*70)\n",
        "print(f\" Settings: 15 sentences | 5 seeds | 30 LIME runs | 1000 samples\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "n_sentences = 15\n",
        "num_samples = 1000\n",
        "n_runs = 30\n",
        "\n",
        "models = {\n",
        "    'Logistic Regression': model,\n",
        "    'DistilBERT': bert_model\n",
        "}\n",
        "\n",
        "exp3_all_seeds = []\n",
        "\n",
        "for seed in SEEDS:\n",
        "    print(f\"\\n{'='*70}\")\n",
        "    print(f\" Running with SEED = {seed}\")\n",
        "    print(f\"{'='*70}\")\n",
        "\n",
        "    np.random.seed(seed)\n",
        "\n",
        "    # Same sentences for both models\n",
        "    exp3_sentences = test_df.sample(n=n_sentences, random_state=seed)\n",
        "\n",
        "    for model_name, model_obj in models.items():\n",
        "        print(f\"\\n   Testing: {model_name}\")\n",
        "\n",
        "        # Create analyzer for this model\n",
        "        analyzer_exp3 = LIMEStabilityAnalyzer(model_obj)\n",
        "\n",
        "        top3_list = []\n",
        "        corr_list = []\n",
        "        cv_list = []\n",
        "\n",
        "        for idx, row in tqdm(exp3_sentences.iterrows(), total=len(exp3_sentences),\n",
        "                             desc=model_name, leave=False):\n",
        "            explanations = analyzer_exp3.explain_multiple(row['sentence'],\n",
        "                                                          num_samples=num_samples,\n",
        "                                                          num_runs=n_runs)\n",
        "\n",
        "            top3 = metrics_calc.top_k_agreement(explanations, k=3)\n",
        "            corr = metrics_calc.rank_correlation(explanations)\n",
        "            cv = metrics_calc.coefficient_of_variation(explanations)\n",
        "\n",
        "            top3_list.append(top3)\n",
        "            corr_list.append(corr)\n",
        "            cv_list.append(cv)\n",
        "\n",
        "        exp3_all_seeds.append({\n",
        "            'seed': seed,\n",
        "            'model': model_name,\n",
        "            'top3_agreement': np.mean(top3_list),\n",
        "            'rank_correlation': np.mean(corr_list),\n",
        "            'coeff_variation': np.mean(cv_list)\n",
        "        })\n",
        "\n",
        "        print(f\"    Top-3: {np.mean(top3_list):.3f} | Corr: {np.mean(corr_list):.3f} | CV: {np.mean(cv_list):.3f}\")\n",
        "\n",
        "# Aggregate across seeds\n",
        "exp3_raw_df = pd.DataFrame(exp3_all_seeds)\n",
        "\n",
        "exp3_results = exp3_raw_df.groupby('model').agg({\n",
        "    'top3_agreement': ['mean', 'std'],\n",
        "    'rank_correlation': ['mean', 'std'],\n",
        "    'coeff_variation': ['mean', 'std']\n",
        "}).reset_index()\n",
        "\n",
        "exp3_results.columns = ['model', 'top3_mean', 'top3_std', 'corr_mean', 'corr_std', 'cv_mean', 'cv_std']\n",
        "\n",
        "print(f\"\\n{'='*70}\")\n",
        "print(\" EXPERIMENT 3 RESULTS (Averaged across 5 seeds):\")\n",
        "print(f\"{'='*70}\")\n",
        "print(exp3_results.to_string(index=False))\n",
        "\n",
        "# Save\n",
        "exp3_raw_df.to_csv('exp3_raw_results.csv', index=False)\n",
        "exp3_results.to_csv('exp3_aggregated_results.csv', index=False)"
      ],
      "metadata": {
        "id": "8uMr_t9LPTdg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
        "\n",
        "colors = ['#2E86AB', '#F18F01']\n",
        "model_names = exp3_results['model'].tolist()\n",
        "x_pos = np.arange(len(model_names))\n",
        "\n",
        "# Plot 1: Top-3 Agreement\n",
        "bars1 = axes[0].bar(x_pos, exp3_results['top3_mean'], yerr=exp3_results['top3_std'],\n",
        "                    color=colors, edgecolor='black', linewidth=1.2, capsize=5)\n",
        "axes[0].set_xticks(x_pos)\n",
        "axes[0].set_xticklabels(model_names)\n",
        "axes[0].set_ylabel('Top-3 Agreement', fontsize=12)\n",
        "axes[0].set_title('Top-3 Agreement by Model', fontsize=14, fontweight='bold')\n",
        "axes[0].set_ylim(0, 1.1)\n",
        "axes[0].axhline(y=0.9, color='green', linestyle='--', alpha=0.5)\n",
        "\n",
        "for bar, val in zip(bars1, exp3_results['top3_mean']):\n",
        "    axes[0].text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.05,\n",
        "                 f'{val:.3f}', ha='center', va='bottom', fontsize=11, fontweight='bold')\n",
        "\n",
        "# Plot 2: Rank Correlation\n",
        "bars2 = axes[1].bar(x_pos, exp3_results['corr_mean'], yerr=exp3_results['corr_std'],\n",
        "                    color=colors, edgecolor='black', linewidth=1.2, capsize=5)\n",
        "axes[1].set_xticks(x_pos)\n",
        "axes[1].set_xticklabels(model_names)\n",
        "axes[1].set_ylabel('Rank Correlation', fontsize=12)\n",
        "axes[1].set_title('Rank Correlation by Model', fontsize=14, fontweight='bold')\n",
        "axes[1].set_ylim(0, 1.1)\n",
        "axes[1].axhline(y=0.9, color='green', linestyle='--', alpha=0.5)\n",
        "\n",
        "for bar, val in zip(bars2, exp3_results['corr_mean']):\n",
        "    axes[1].text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.05,\n",
        "                 f'{val:.3f}', ha='center', va='bottom', fontsize=11, fontweight='bold')\n",
        "\n",
        "# Plot 3: Coefficient of Variation\n",
        "bars3 = axes[2].bar(x_pos, exp3_results['cv_mean'], yerr=exp3_results['cv_std'],\n",
        "                    color=colors, edgecolor='black', linewidth=1.2, capsize=5)\n",
        "axes[2].set_xticks(x_pos)\n",
        "axes[2].set_xticklabels(model_names)\n",
        "axes[2].set_ylabel('Coefficient of Variation', fontsize=12)\n",
        "axes[2].set_title('Score Variation by Model', fontsize=14, fontweight='bold')\n",
        "axes[2].axhline(y=0.5, color='green', linestyle='--', alpha=0.5)\n",
        "\n",
        "for bar, val in zip(bars3, exp3_results['cv_mean']):\n",
        "    axes[2].text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.05,\n",
        "                 f'{val:.3f}', ha='center', va='bottom', fontsize=11, fontweight='bold')\n",
        "\n",
        "plt.suptitle('EXPERIMENT 3: Effect of Model Complexity on LIME Stability (5 seeds)',\n",
        "             fontsize=16, fontweight='bold', y=1.02)\n",
        "plt.tight_layout()\n",
        "plt.savefig('exp3_model_vs_stability.png', dpi=300, bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "print(\" Visualization saved to 'exp3_model_vs_stability.png'\")"
      ],
      "metadata": {
        "id": "DDOI-Eb4XUT2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fig3, ax3 = plt.subplots(figsize=(10, 6))\n",
        "\n",
        "# Data\n",
        "models = ['Logistic Regression\\n(Simple Model)', 'DistilBERT\\n(Complex Model)']\n",
        "agreement_model = [0.947, 0.848]\n",
        "std_model = [0.015, 0.041]\n",
        "\n",
        "# Create horizontal bullet chart style\n",
        "y_pos = [1.5, 0.5]\n",
        "bar_height = 0.6\n",
        "\n",
        "# Background bars (full scale to 1.0)\n",
        "ax3.barh(y_pos, [1.0, 1.0], height=bar_height, color=COLORS['light_gray'],\n",
        "         edgecolor=COLORS['gray'], linewidth=2)\n",
        "\n",
        "# Actual value bars\n",
        "colors_model = [COLORS['green'], COLORS['red']]\n",
        "for i, (y, val, std, color) in enumerate(zip(y_pos, agreement_model, std_model, colors_model)):\n",
        "    ax3.barh(y, val, height=bar_height, color=color, edgecolor='white', linewidth=2)\n",
        "\n",
        "    # Add value label inside bar\n",
        "    ax3.text(val - 0.03, y, f'{val:.3f}', ha='right', va='center',\n",
        "             fontsize=16, fontweight='bold', color='white')\n",
        "\n",
        "    # Add unreliability label outside\n",
        "    unreliable = (1 - val) * 100\n",
        "    ax3.text(val + 0.02, y, f'({unreliable:.1f}% unreliable)', ha='left', va='center',\n",
        "             fontsize=12, fontweight='bold',\n",
        "             color=COLORS['red'] if unreliable > 10 else COLORS['gray'])\n",
        "\n",
        "# Model labels\n",
        "ax3.set_yticks(y_pos)\n",
        "ax3.set_yticklabels(models, fontsize=13, fontweight='bold')\n",
        "\n",
        "# Add drop annotation with arrow between bars\n",
        "ax3.annotate('', xy=(0.848, 0.85), xytext=(0.947, 1.15),\n",
        "            arrowprops=dict(arrowstyle='->', color=COLORS['red'], lw=3,\n",
        "                           connectionstyle='arc3,rad=-0.2'))\n",
        "ax3.text(0.85, 1.0, '~10%\\nDROP!', ha='center', va='center', fontsize=14,\n",
        "         fontweight='bold', color=COLORS['red'],\n",
        "         bbox=dict(boxstyle='round,pad=0.4', facecolor='#FADBD8', edgecolor=COLORS['red'], linewidth=2))\n",
        "\n",
        "# Threshold markers\n",
        "ax3.axvline(x=0.95, color=COLORS['orange'], linestyle=':', linewidth=2, label='Acceptable (0.95)')\n",
        "ax3.axvline(x=1.0, color=COLORS['green'], linestyle='--', linewidth=2, label='Perfect (1.0)')\n",
        "\n",
        "# Styling\n",
        "ax3.set_xlim(0.75, 1.08)\n",
        "ax3.set_ylim(0, 2)\n",
        "ax3.set_xlabel('Top-3 Agreement (Reliability)', fontsize=14, fontweight='bold')\n",
        "ax3.set_title('Exp 3: Complex Models Significantly Reduce LIME Reliability\\n(p = 0.0009, t-test)',\n",
        "              fontsize=16, fontweight='bold', color=COLORS['dark_blue'], pad=15)\n",
        "ax3.legend(loc='lower right', fontsize=10)\n",
        "\n",
        "# Remove y-axis spine\n",
        "ax3.spines['left'].set_visible(False)\n",
        "ax3.tick_params(axis='y', length=0)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('exp3_model_complexity.png', dpi=300, bbox_inches='tight', facecolor='white')\n",
        "plt.close()\n",
        "print(\"✓ Saved: exp3_model_complexity.png\")\n",
        "\n"
      ],
      "metadata": {
        "id": "tsgsIaa5f0JY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"=\"*70)\n",
        "print(\"🔬 EXPERIMENT 4: Feature Effects Stability Analysis\")\n",
        "print(\"=\"*70)\n",
        "print(\"Analyzing stability of effect MAGNITUDES, not just rankings\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# This addresses professor's feedback about \"local feature effects\"\n",
        "# We analyze if the actual importance SCORES are stable, not just which words rank highest\n",
        "\n",
        "def analyze_feature_effects(explanations):\n",
        "    \"\"\"\n",
        "    Analyze stability of feature effect magnitudes across LIME runs.\n",
        "\n",
        "    Returns:\n",
        "    - sign_consistency: % of runs where word has same sign (positive/negative)\n",
        "    - magnitude_stability: How consistent are the actual coefficient values\n",
        "    \"\"\"\n",
        "    all_words = set()\n",
        "    for exp in explanations:\n",
        "        all_words.update(exp.keys())\n",
        "\n",
        "    results = {}\n",
        "    for word in all_words:\n",
        "        scores = [exp.get(word, 0) for exp in explanations if word in exp]\n",
        "        if len(scores) < 2:\n",
        "            continue\n",
        "\n",
        "        # Sign consistency: do all runs agree on positive/negative effect?\n",
        "        signs = [1 if s > 0 else -1 for s in scores]\n",
        "        sign_consistency = max(signs.count(1), signs.count(-1)) / len(signs)\n",
        "\n",
        "        # Magnitude stability: coefficient of variation of absolute scores\n",
        "        abs_scores = [abs(s) for s in scores]\n",
        "        if np.mean(abs_scores) > 0:\n",
        "            magnitude_cv = np.std(abs_scores) / np.mean(abs_scores)\n",
        "        else:\n",
        "            magnitude_cv = 0\n",
        "\n",
        "        results[word] = {\n",
        "            'sign_consistency': sign_consistency,\n",
        "            'magnitude_cv': magnitude_cv,\n",
        "            'mean_score': np.mean(scores),\n",
        "            'std_score': np.std(scores)\n",
        "        }\n",
        "\n",
        "    return results\n",
        "\n",
        "# Run feature effects analysis on sample sentences\n",
        "n_sentences_fe = 20\n",
        "num_samples = 1000\n",
        "n_runs = 30\n",
        "\n",
        "fe_results_all = []\n",
        "\n",
        "for seed in SEEDS[:3]:  # Use 3 seeds for faster computation\n",
        "    print(f\"\\n Seed = {seed}\")\n",
        "    np.random.seed(seed)\n",
        "\n",
        "    sample_sentences = test_df.sample(n=n_sentences_fe, random_state=seed)\n",
        "\n",
        "    for idx, row in tqdm(sample_sentences.iterrows(), total=len(sample_sentences)):\n",
        "        explanations = analyzer.explain_multiple(row['sentence'],\n",
        "                                                 num_samples=num_samples,\n",
        "                                                 num_runs=n_runs)\n",
        "\n",
        "        fe_analysis = analyze_feature_effects(explanations)\n",
        "\n",
        "        # Aggregate per sentence\n",
        "        if fe_analysis:\n",
        "            avg_sign_consistency = np.mean([v['sign_consistency'] for v in fe_analysis.values()])\n",
        "            avg_magnitude_cv = np.mean([v['magnitude_cv'] for v in fe_analysis.values()])\n",
        "\n",
        "            fe_results_all.append({\n",
        "                'seed': seed,\n",
        "                'sentence_length': len(row['sentence'].split()),\n",
        "                'sign_consistency': avg_sign_consistency,\n",
        "                'magnitude_cv': avg_magnitude_cv\n",
        "            })\n",
        "\n",
        "fe_df = pd.DataFrame(fe_results_all)\n",
        "\n",
        "print(f\"\\n{'='*70}\")\n",
        "print(\"📊 FEATURE EFFECTS STABILITY RESULTS:\")\n",
        "print(f\"{'='*70}\")\n",
        "print(f\"\\nSign Consistency (do words keep same +/- direction?):\")\n",
        "print(f\"   Mean: {fe_df['sign_consistency'].mean():.3f} ± {fe_df['sign_consistency'].std():.3f}\")\n",
        "print(f\"\\nMagnitude CV (how stable are actual scores?):\")\n",
        "print(f\"   Mean: {fe_df['magnitude_cv'].mean():.3f} ± {fe_df['magnitude_cv'].std():.3f}\")"
      ],
      "metadata": {
        "id": "wkNtXC6objco"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualization\n",
        "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
        "\n",
        "# Plot 1: Sign Consistency Distribution\n",
        "axes[0].hist(fe_df['sign_consistency'], bins=20, color='#2E86AB', edgecolor='black', alpha=0.7)\n",
        "axes[0].axvline(x=fe_df['sign_consistency'].mean(), color='red', linestyle='--',\n",
        "                label=f\"Mean: {fe_df['sign_consistency'].mean():.3f}\")\n",
        "axes[0].set_xlabel('Sign Consistency', fontsize=12)\n",
        "axes[0].set_ylabel('Frequency', fontsize=12)\n",
        "axes[0].set_title('Distribution of Sign Consistency Across Sentences', fontsize=14, fontweight='bold')\n",
        "axes[0].legend()\n",
        "\n",
        "# Plot 2: Magnitude CV Distribution\n",
        "axes[1].hist(fe_df['magnitude_cv'], bins=20, color='#F18F01', edgecolor='black', alpha=0.7)\n",
        "axes[1].axvline(x=fe_df['magnitude_cv'].mean(), color='red', linestyle='--',\n",
        "                label=f\"Mean: {fe_df['magnitude_cv'].mean():.3f}\")\n",
        "axes[1].set_xlabel('Magnitude Coefficient of Variation', fontsize=12)\n",
        "axes[1].set_ylabel('Frequency', fontsize=12)\n",
        "axes[1].set_title('Distribution of Magnitude Stability Across Sentences', fontsize=14, fontweight='bold')\n",
        "axes[1].legend()\n",
        "\n",
        "plt.suptitle('EXPERIMENT 4: Feature Effects Stability Analysis', fontsize=16, fontweight='bold', y=1.02)\n",
        "plt.tight_layout()\n",
        "plt.savefig('exp4_feature_effects_stability.png', dpi=300, bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "print(\"\\n Feature effects analysis complete!\")\n",
        "print(\"   This addresses the question: Are effect MAGNITUDES stable, not just rankings?\")\n",
        "plt.savefig('exp4_feature_effects.png', dpi=300, bbox_inches='tight')\n"
      ],
      "metadata": {
        "id": "gR07DWjjicNE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fig4, axes4 = plt.subplots(1, 2, figsize=(12, 5))\n",
        "\n",
        "# Left: Sign Consistency (Donut)\n",
        "ax4a = axes4[0]\n",
        "sign_consistent = 95.7\n",
        "sign_inconsistent = 100 - sign_consistent\n",
        "\n",
        "# Create donut chart\n",
        "colors_donut1 = [COLORS['green'], COLORS['light_gray']]\n",
        "wedges1, texts1 = ax4a.pie([sign_consistent, sign_inconsistent], colors=colors_donut1,\n",
        "                           startangle=90, wedgeprops=dict(width=0.4, edgecolor='white', linewidth=3))\n",
        "\n",
        "# Center text\n",
        "ax4a.text(0, 0, f'{sign_consistent:.1f}%', ha='center', va='center',\n",
        "          fontsize=32, fontweight='bold', color=COLORS['green'])\n",
        "ax4a.text(0, -0.15, 'Consistent', ha='center', va='top',\n",
        "          fontsize=12, color=COLORS['dark'])\n",
        "\n",
        "ax4a.set_title('Sign Consistency\\n(Direction of Effect)', fontsize=14, fontweight='bold',\n",
        "               color=COLORS['dark_blue'], pad=10)\n",
        "\n",
        "# Add label\n",
        "ax4a.text(0, -0.6, ' GOOD: Words keep their +/- direction', ha='center',\n",
        "          fontsize=11, fontweight='bold', color=COLORS['green'],\n",
        "          bbox=dict(boxstyle='round,pad=0.4', facecolor='#D5F5E3', edgecolor=COLORS['green']))\n",
        "\n",
        "# Right: Magnitude Variation (Donut showing CV)\n",
        "ax4b = axes4[1]\n",
        "cv_magnitude = 14.9  # This represents the coefficient of variation\n",
        "cv_rest = 100 - cv_magnitude\n",
        "\n",
        "# Create donut chart - but showing this as \"variation\" which is bad\n",
        "colors_donut2 = [COLORS['orange'], COLORS['light_gray']]\n",
        "wedges2, texts2 = ax4b.pie([cv_magnitude, cv_rest], colors=colors_donut2,\n",
        "                           startangle=90, wedgeprops=dict(width=0.4, edgecolor='white', linewidth=3))\n",
        "\n",
        "# Center text\n",
        "ax4b.text(0, 0, f'{cv_magnitude:.1f}%', ha='center', va='center',\n",
        "          fontsize=32, fontweight='bold', color=COLORS['orange'])\n",
        "ax4b.text(0, -0.15, 'Variation (CV)', ha='center', va='top',\n",
        "          fontsize=12, color=COLORS['dark'])\n",
        "\n",
        "ax4b.set_title('Magnitude Variation\\n(Importance Scores)', fontsize=14, fontweight='bold',\n",
        "               color=COLORS['dark_blue'], pad=10)\n",
        "\n",
        "# Add label\n",
        "ax4b.text(0, -0.6, '⚠ CAUTION: Importance values fluctuate', ha='center',\n",
        "          fontsize=11, fontweight='bold', color=COLORS['orange'],\n",
        "          bbox=dict(boxstyle='round,pad=0.4', facecolor='#FCF3CF', edgecolor=COLORS['orange']))\n",
        "\n",
        "# Overall title\n",
        "fig4.suptitle('Exp 4: Feature Effects Are Directionally Stable, But Magnitudes Vary',\n",
        "              fontsize=16, fontweight='bold', color=COLORS['dark_blue'], y=1.02)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('exp4_feature_effects.png', dpi=300, bbox_inches='tight', facecolor='white')\n",
        "plt.close()\n",
        "print(\" Saved: exp4_feature_effects.png\")\n"
      ],
      "metadata": {
        "id": "zChVtzzDgNB5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- CODE CELL ---\n",
        "from scipy.stats import ttest_ind, f_oneway, pearsonr\n",
        "\n",
        "print(\"=\"*70)\n",
        "print(\" STATISTICAL SIGNIFICANCE TESTS\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# Test 1: Is Logistic Regression significantly more stable than DistilBERT?\n",
        "print(\"\\n\" + \"-\"*70)\n",
        "print(\"TEST 1: Logistic Regression vs DistilBERT Stability\")\n",
        "print(\"-\"*70)\n",
        "\n",
        "logreg_top3 = exp3_raw_df[exp3_raw_df['model']=='Logistic Regression']['top3_agreement'].values\n",
        "bert_top3 = exp3_raw_df[exp3_raw_df['model']=='DistilBERT']['top3_agreement'].values\n",
        "\n",
        "t_stat, p_value = ttest_ind(logreg_top3, bert_top3)\n",
        "print(f\"   Logistic Regression Top-3: {logreg_top3.mean():.3f} ± {logreg_top3.std():.3f}\")\n",
        "print(f\"   DistilBERT Top-3: {bert_top3.mean():.3f} ± {bert_top3.std():.3f}\")\n",
        "print(f\"   t-statistic: {t_stat:.3f}\")\n",
        "print(f\"   p-value: {p_value:.4f}\")\n",
        "print(f\"   Result: {' SIGNIFICANT (p < 0.05)' if p_value < 0.05 else ' Not significant'}\")\n",
        "\n",
        "# Test 2: Does num_samples significantly affect stability? (ANOVA)\n",
        "print(\"\\n\" + \"-\"*70)\n",
        "print(\"TEST 2: Effect of num_samples (One-way ANOVA)\")\n",
        "print(\"-\"*70)\n",
        "\n",
        "groups = [exp1_raw_df[exp1_raw_df['num_samples']==ns]['top3_agreement'].values\n",
        "          for ns in [100, 250, 500, 1000, 2000]]\n",
        "f_stat, p_value_anova = f_oneway(*groups)\n",
        "print(f\"   F-statistic: {f_stat:.3f}\")\n",
        "print(f\"   p-value: {p_value_anova:.6f}\")\n",
        "print(f\"   Result: {' SIGNIFICANT (p < 0.05)' if p_value_anova < 0.05 else ' Not significant'}\")\n",
        "\n",
        "# Test 3: Correlation between num_samples and stability\n",
        "print(\"\\n\" + \"-\"*70)\n",
        "print(\"TEST 3: Correlation between num_samples and Top-3 Agreement\")\n",
        "print(\"-\"*70)\n",
        "\n",
        "corr, p_corr = pearsonr(exp1_raw_df['num_samples'], exp1_raw_df['top3_agreement'])\n",
        "print(f\"   Pearson correlation: {corr:.3f}\")\n",
        "print(f\"   p-value: {p_corr:.6f}\")\n",
        "print(f\"   Result: {' SIGNIFICANT' if p_corr < 0.05 else ' Not significant'} positive correlation\")"
      ],
      "metadata": {
        "id": "A8-eMFLLikwE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n\" + \"-\"*70)\n",
        "print(\"TEST 4: Effect of Sentence Length (One-way ANOVA)\")\n",
        "print(\"-\"*70)\n",
        "\n",
        "short_scores = exp2_raw_df[exp2_raw_df['group']=='Short (≤7 words)']['top3_agreement'].values\n",
        "medium_scores = exp2_raw_df[exp2_raw_df['group']=='Medium (8-15 words)']['top3_agreement'].values\n",
        "long_scores = exp2_raw_df[exp2_raw_df['group']=='Long (>15 words)']['top3_agreement'].values\n",
        "\n",
        "f_stat_len, p_value_len = f_oneway(short_scores, medium_scores, long_scores)\n",
        "print(f\"   Short: {short_scores.mean():.3f} | Medium: {medium_scores.mean():.3f} | Long: {long_scores.mean():.3f}\")\n",
        "print(f\"   F-statistic: {f_stat_len:.3f}\")\n",
        "print(f\"   p-value: {p_value_len:.4f}\")\n",
        "print(f\"   Result: {'SIGNIFICANT (p < 0.05)' if p_value_len < 0.05 else ' Not significant'}\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\" STATISTICAL TESTS SUMMARY\")\n",
        "print(\"=\"*70)"
      ],
      "metadata": {
        "id": "1KWdKyjVioVK"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}